# -*- coding: utf-8 -*-
"""EE19B094.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gcIFuJWBg06OE_1H_tY0QkgZOeNgKrVK

# General Instructions to students:

1. There are 5 types of cells in this notebook. The cell type will be indicated within the cell.
    1. Markdown cells with problem written in it. (DO NOT TOUCH THESE CELLS) (**Cell type: TextRead**)
    2. Python cells with setup code for further evaluations. (DO NOT TOUCH THESE CELLS) (**Cell type: CodeRead**)
    3. Python code cells with some template code or empty cell. (FILL CODE IN THESE CELLS BASED ON INSTRUCTIONS IN CURRENT AND PREVIOUS CELLS) (**Cell type: CodeWrite**)
    4. Markdown cells where a written reasoning or conclusion is expected. (WRITE SENTENCES IN THESE CELLS) (**Cell type: TextWrite**)
    5. Temporary code cells for convenience and TAs. (YOU MAY DO WHAT YOU WILL WITH THESE CELLS, TAs WILL REPLACE WHATEVER YOU WRITE HERE WITH OFFICIAL EVALUATION CODE) (**Cell type: Convenience**)
    
2. You are not allowed to insert new cells in the submitted notebook.

3. You are not allowed to **import** any extra packages.

4. The code is to be written in Python 3.6 syntax. Latest versions of other packages maybe assumed.

5. In CodeWrite Cells, the only outputs to be given are plots asked in the question. Nothing else to be output/print. 

6. If TextWrite cells ask you to give accuracy/error/other numbers you can print them on the code cells, but remove the print statements before submitting.

7. The convenience code can be used to check the expected syntax of the functions. At a minimum, your entire notebook must run with "run all" with the convenience cells as it is. Any runtime failures on the submitted notebook as it is will get zero marks.

8. All code must be written by yourself. Copying from other students/material on the web is strictly prohibited. Any violations will result in zero marks.

9. All datasets will be given as .npz files, and will contain data in 4 numpy arrays :"X_train, Y_train, X_test, Y_test". In that order. The meaning of the 4 arrays can be easily inferred from their names.

10. All plots must be labelled properly, all tables must have rows and columns named properly.

11. Change the name of file with your roll no.
"""

# Cell type : CodeRead

import numpy as np
import matplotlib.pyplot as plt

"""**Cell type : TextRead**

# Problem 6: Learning Binary Bayes Classifiers from data with Max. Likelihood 

Derive Bayes classifiers under assumptions below, and use ML estimators to compute and return the results on a test set. 

BayesA) Assume $X|Y=-1 \sim \mathcal{N}(\mu_-, I)$ and  $X|Y=1 \sim \mathcal{N}(\mu_+, I)$. *(Same known covariance)*

BayesB) Assume $X|Y=-1 \sim \mathcal{N}(\mu_-, \Sigma)$ and $X|Y=1 \sim \mathcal{N}(\mu_+, \Sigma)$ *(Same unknown covariance)*

BayesC) Assume $X|Y=-1 \sim \mathcal{N}(\mu_-, \Sigma_-)$ and $X|Y=1 \sim \mathcal{N}(\mu_+, \Sigma_+)$ *(different unknown covariance)*



"""

# Cell type : CodeWrite

def function_for_A(X_train, Y_train, X_test):
    feature_dimension = int(np.shape(X_train[0])[0])
    negative_count = 0
    positive_count = 0
    for Y in Y_train:
    	if Y == -1:
        	negative_count += 1
    	else:
        	positive_count += 1


    X_train_neg = np.empty([0, feature_dimension],dtype=float)
    X_train_pos = np.empty([0, feature_dimension],dtype=float)
    for (X,Y) in zip(X_train,Y_train):
      	if Y == -1.0:
        	X_train_neg = np.vstack([X_train_neg,X])
      	else:
        	X_train_pos = np.vstack([X_train_pos,X])
    

    mean_positive = np.zeros([1,feature_dimension])
    for X in X_train_pos:
      	mean_positive = np.add(mean_positive,X)
    mean_positive = mean_positive/positive_count

    mean_negative = np.zeros([1,feature_dimension])
    for X in X_train_neg:
      	mean_negative = np.add(mean_negative,X)
    mean_negative = mean_negative/negative_count

    Y_test = np.empty([0],dtype=float)
    for X in X_test:
      	X_mean = np.add(X,-1*mean_positive)
      	X_value = np.matmul(X_mean,np.transpose(X_mean))
      	pos_class_prob = np.power((1/(2*np.pi)),feature_dimension/2)*np.exp((X_value)*(-1/2))

      	X_mean = np.add(X,-1*mean_negative)
      	X_value = np.matmul(X_mean,np.transpose(X_mean))
      	neg_class_prob = np.power((1/(2*np.pi)),feature_dimension/2)*np.exp((X_value)*(-1/2))

      	pos_posterior = (positive_count/(positive_count+negative_count))*pos_class_prob
      	neg_posterior = (negative_count/(positive_count+negative_count))*neg_class_prob

      	if pos_posterior >= neg_posterior :
        	Y_test = np.append(Y_test,1)
      	else:
        	Y_test = np.append(Y_test,-1)
      
    return Y_test
    
def function_for_B(X_train, Y_train, X_test):
    feature_dimension = int(np.shape(X_train[0])[0])
    negative_count = 0
    positive_count = 0
    for Y in Y_train:
    	if Y == -1:
        	negative_count += 1
    	else:
        	positive_count += 1

 
    X_train_neg = np.empty([0, feature_dimension],dtype=float)
    X_train_pos = np.empty([0, feature_dimension],dtype=float)
    for (X,Y) in zip(X_train,Y_train):
      	if Y == -1.0:
        	X_train_neg = np.vstack([X_train_neg,X])
      	else:
        	X_train_pos = np.vstack([X_train_pos,X])

    
    mean_positive = np.zeros([1,feature_dimension])
    for X in X_train_pos:
      	mean_positive = np.add(mean_positive,X)
    mean_positive = mean_positive/positive_count

    mean_negative = np.zeros([1,feature_dimension])
    for X in X_train_neg:
      	mean_negative = np.add(mean_negative,X)
    mean_negative = mean_negative/negative_count

    covariance_matrix = np.cov(np.transpose(X_train))
    covariance_inverse = np.linalg.inv(covariance_matrix)

    Y_test = np.empty([0],dtype=float)
    for X in X_test:
      	X_mean = np.add(X,-1*mean_positive)
      	X_value = np.matmul(X_mean,covariance_inverse)
      	X_value = np.matmul(X_value,np.transpose(X_mean))
      	pos_class_prob = np.power((1/(2*np.pi)),feature_dimension/2)*np.exp((X_value)*(-1/2))*np.power(np.linalg.det(covariance_matrix),-(1/2))

      	X_mean = np.add(X,-1*mean_negative)
      	X_value = np.matmul(X_mean,covariance_inverse)
      	X_value = np.matmul(X_value,np.transpose(X_mean))
      	neg_class_prob = np.power((1/(2*np.pi)),feature_dimension/2)*np.exp((X_value)*(-1/2))*np.power(np.linalg.det(covariance_matrix),-(1/2))

      	pos_posterior = (positive_count/(positive_count+negative_count))*pos_class_prob
      	neg_posterior = (negative_count/(positive_count+negative_count))*neg_class_prob

      	if pos_posterior >= neg_posterior :
        	Y_test = np.append(Y_test,1)
      	else:
        	Y_test = np.append(Y_test,-1)
      
    return Y_test

def function_for_C(X_train, Y_train, X_test):
    feature_dimension = int(np.shape(X_train[0])[0])
    negative_count = 0
    positive_count = 0
    for Y in Y_train:
    	if Y == -1:
        	negative_count += 1
    	else:
        	positive_count += 1

  
    X_train_neg = np.empty([0, feature_dimension],dtype=float)
    X_train_pos = np.empty([0, feature_dimension],dtype=float)
    for (X,Y) in zip(X_train,Y_train):
      	if Y == -1.0:
        	X_train_neg = np.vstack([X_train_neg,X])
      	else:
        	X_train_pos = np.vstack([X_train_pos,X])
    
    mean_positive = np.zeros([1,feature_dimension])
    for X in X_train_pos:
      	mean_positive = np.add(mean_positive,X)
    mean_positive = mean_positive/positive_count

    mean_negative = np.zeros([1,feature_dimension])
    for X in X_train_neg:
      	mean_negative = np.add(mean_negative,X)
    mean_negative = mean_negative/negative_count

    covariance_matrix_pos = np.cov(np.transpose(X_train_pos))
    covariance_inverse_pos = np.linalg.inv(covariance_matrix_pos)

    covariance_matrix_neg = np.cov(np.transpose(X_train_neg))
    covariance_inverse_neg = np.linalg.inv(covariance_matrix_neg)

    Y_test = np.empty([0],dtype=float)
    for X in X_test:
      	X_mean = np.add(X,-1*mean_positive)
      	X_value = np.matmul(X_mean,covariance_inverse_pos)
      	X_value = np.matmul(X_value,np.transpose(X_mean))
      	pos_class_prob = np.power((1/(2*np.pi)),feature_dimension/2)*np.exp((X_value)*(-1/2))*np.power(np.linalg.det(covariance_matrix_pos),-(1/2))

      	X_mean = np.add(X,-1*mean_negative)
      	X_value = np.matmul(X_mean,covariance_inverse_neg)
      	X_value = np.matmul(X_value,np.transpose(X_mean))
      	neg_class_prob = np.power((1/(2*np.pi)),feature_dimension/2)*np.exp((X_value)*(-1/2))*np.power(np.linalg.det(covariance_matrix_neg),-(1/2))

      	pos_posterior = (positive_count/(positive_count+negative_count))*pos_class_prob
      	neg_posterior = (negative_count/(positive_count+negative_count))*neg_class_prob

      	if pos_posterior >= neg_posterior :
        	Y_test = np.append(Y_test,1)
      	else:
        	Y_test = np.append(Y_test,-1)
      
    return Y_test

# Cell type : Convenience

# Testing the functions above

# To students: You may use the example here for testing syntax issues 
# with your functions, and also as a sanity check. But the final evaluation
# will be done for different inputs to the functions. (So you can't just 
# solve the problem for this one example given below.) 
# try to remove everything or comment out your lines before submitting.


X_train_pos = np.random.randn(1000,2)+np.array([[1.,2.]])
X_train_neg = np.random.randn(1000,2)+np.array([[2.,4.]])
X_train = np.concatenate((X_train_pos, X_train_neg), axis=0)
Y_train = np.concatenate(( np.ones(1000), -1*np.ones(1000) ))
X_test_pos = np.random.randn(1000,2)+np.array([[1.,2.]])
X_test_neg = np.random.randn(1000,2)+np.array([[2.,4.]])
X_test = np.concatenate((X_test_pos, X_test_neg), axis=0)
Y_test = np.concatenate(( np.ones(1000), -1*np.ones(1000) ))

Y_pred_test_1a = function_for_A(X_train, Y_train, X_test)
Y_pred_test_1b = function_for_B(X_train, Y_train, X_test)
Y_pred_test_1c = function_for_C(X_train, Y_train, X_test)

"""**Cell type : TextRead**

# Problem 6

#### 6a) Run the above three algorithms (BayesA,B and C), for the three datasets given (datasetA.npz, datasetB.npz, datasetC.npz) in the cell below.
#### In the next CodeWrite cell, Plot all the classifiers (3 classification algos on 3 datasets = 9 plots) on a 2d plot (color the positively classified area light green, and negatively classified area light red). Add the training data points also on the plot. Plots to be organised into 3 plots follows: One plot for each dataset, with three subplots in each for the three classifiers. Label the 9 plots appropriately.




"""

# Cell type : CodeWrite
# write the code for loading the data, running the three algos, and plotting here. 
# (Use the functions written previously.)
file_1 = np.load('/content/datasetA.npz')
Y_test_A_1 = function_for_A(file_1['arr_0.npy'],file_1['arr_1.npy'],file_1['arr_2.npy'])
Y_test_B_1 = function_for_B(file_1['arr_0.npy'],file_1['arr_1.npy'],file_1['arr_2.npy'])
Y_test_C_1 = function_for_C(file_1['arr_0.npy'],file_1['arr_1.npy'],file_1['arr_2.npy'])
file_2 = np.load('/content/datasetB.npz')
Y_test_A_2 = function_for_A(file_2['arr_0.npy'],file_2['arr_1.npy'],file_2['arr_2.npy'])
Y_test_B_2 = function_for_B(file_2['arr_0.npy'],file_2['arr_1.npy'],file_2['arr_2.npy'])
Y_test_C_2 = function_for_C(file_2['arr_0.npy'],file_2['arr_1.npy'],file_2['arr_2.npy'])
file_3 = np.load('/content/datasetC.npz')
Y_test_A_3 = function_for_A(file_3['arr_0.npy'],file_3['arr_1.npy'],file_3['arr_2.npy'])
Y_test_B_3 = function_for_B(file_3['arr_0.npy'],file_3['arr_1.npy'],file_3['arr_2.npy'])
Y_test_C_3 = function_for_C(file_3['arr_0.npy'],file_3['arr_1.npy'],file_3['arr_2.npy'])

def function_for_A_params(X_train,Y_train,X_test,threshold):
    feature_dimension = int(np.shape(X_train[0])[0])
    negative_count = 0
    positive_count = 0
    for Y in Y_train:
    	if Y == -1:
        	negative_count += 1
    	else:
        	positive_count += 1

    X_train_neg = np.empty([0, feature_dimension],dtype=float)
    X_train_pos = np.empty([0, feature_dimension],dtype=float)
    for (X,Y) in zip(X_train,Y_train):
      	if Y == -1.0:
        	X_train_neg = np.vstack([X_train_neg,X])
      	else:
        	X_train_pos = np.vstack([X_train_pos,X])
    

    mean_positive = np.zeros([1,feature_dimension])
    for X in X_train_pos:
      	mean_positive = np.add(mean_positive,X)
    mean_positive = mean_positive/positive_count

    mean_negative = np.zeros([1,feature_dimension])
    for X in X_train_neg:
      	mean_negative = np.add(mean_negative,X)
    mean_negative = mean_negative/negative_count
    total_count = positive_count+negative_count

    Y_test = np.empty([0],dtype=float)
    for X in X_test:
      	X_mean = np.add(X,-1*mean_positive)
      	X_value = np.matmul(X_mean,np.transpose(X_mean))
      	pos_class_prob = np.power((1/(2*np.pi)),feature_dimension/2)*np.exp((X_value)*(-1/2))

      	X_mean = np.add(X,-1*mean_negative)
      	X_value = np.matmul(X_mean,np.transpose(X_mean))
      	neg_class_prob = np.power((1/(2*np.pi)),feature_dimension/2)*np.exp((X_value)*(-1/2))

      	pos_posterior = (positive_count/total_count)*pos_class_prob/((positive_count/total_count*pos_class_prob)+(negative_count/total_count*neg_class_prob))
      	neg_posterior = (negative_count/total_count)*neg_class_prob/((positive_count/total_count*pos_class_prob)+(negative_count/total_count*neg_class_prob))

      	if pos_posterior >= threshold :
        	Y_test = np.append(Y_test,1)
      	else:
        	Y_test = np.append(Y_test,-1)

    return [X_train_pos,X_train_neg,Y_test,mean_positive,mean_negative,positive_count/total_count,negative_count/total_count]

def function_for_B_params(X_train,Y_train,X_test,threshold):
    feature_dimension = int(np.shape(X_train[0])[0])
    negative_count = 0
    positive_count = 0
    for Y in Y_train:
    	if Y == -1:
        	negative_count += 1
    	else:
        	positive_count += 1

 
    X_train_neg = np.empty([0, feature_dimension],dtype=float)
    X_train_pos = np.empty([0, feature_dimension],dtype=float)
    for (X,Y) in zip(X_train,Y_train):
      	if Y == -1.0:
        	X_train_neg = np.vstack([X_train_neg,X])
      	else:
        	X_train_pos = np.vstack([X_train_pos,X])

    
    mean_positive = np.zeros([1,feature_dimension])
    for X in X_train_pos:
      	mean_positive = np.add(mean_positive,X)
    mean_positive = mean_positive/positive_count

    mean_negative = np.zeros([1,feature_dimension])
    for X in X_train_neg:
      	mean_negative = np.add(mean_negative,X)
    mean_negative = mean_negative/negative_count

    covariance_matrix = np.cov(np.transpose(X_train_neg))
    covariance_inverse = np.linalg.inv(covariance_matrix)
    total_count = positive_count+negative_count

    Y_test = np.empty([0],dtype=float)
    for X in X_test:
      	X_mean = np.add(X,-1*mean_positive)
      	X_value = np.matmul(X_mean,covariance_inverse)
      	X_value = np.matmul(X_value,np.transpose(X_mean))
      	pos_class_prob = np.power((1/(2*np.pi)),feature_dimension/2)*np.exp((X_value)*(-1/2))*np.power(np.linalg.det(covariance_matrix),-(1/2))

      	X_mean = np.add(X,-1*mean_negative)
      	X_value = np.matmul(X_mean,covariance_inverse)
      	X_value = np.matmul(X_value,np.transpose(X_mean))
      	neg_class_prob = np.power((1/(2*np.pi)),feature_dimension/2)*np.exp((X_value)*(-1/2))*np.power(np.linalg.det(covariance_matrix),-(1/2))

      	pos_posterior = (positive_count/total_count)*pos_class_prob/((positive_count/total_count*pos_class_prob)+(negative_count/total_count*neg_class_prob))
      	neg_posterior = (negative_count/total_count)*neg_class_prob/((positive_count/total_count*pos_class_prob)+(negative_count/total_count*neg_class_prob))

      	if pos_posterior >= threshold :
        	Y_test = np.append(Y_test,1)
      	else:
        	Y_test = np.append(Y_test,-1)

    return [X_train_pos,X_train_neg,Y_test,mean_positive,mean_negative,positive_count/total_count,negative_count/total_count,covariance_matrix] 

def function_for_C_params(X_train,Y_train,X_test,threshold):
    feature_dimension = int(np.shape(X_train[0])[0])
    negative_count = 0
    positive_count = 0
    for Y in Y_train:
    	if Y == -1:
        	negative_count += 1
    	else:
        	positive_count += 1

  
    X_train_neg = np.empty([0, feature_dimension],dtype=float)
    X_train_pos = np.empty([0, feature_dimension],dtype=float)
    for (X,Y) in zip(X_train,Y_train):
      	if Y == -1.0:
        	X_train_neg = np.vstack([X_train_neg,X])
      	else:
        	X_train_pos = np.vstack([X_train_pos,X])
    
    mean_positive = np.zeros([1,feature_dimension])
    for X in X_train_pos:
      	mean_positive = np.add(mean_positive,X)
    mean_positive = mean_positive/positive_count

    mean_negative = np.zeros([1,feature_dimension])
    for X in X_train_neg:
      	mean_negative = np.add(mean_negative,X)
    mean_negative = mean_negative/negative_count

    covariance_matrix_pos = np.cov(np.transpose(X_train_pos))
    covariance_inverse_pos = np.linalg.inv(covariance_matrix_pos)

    covariance_matrix_neg = np.cov(np.transpose(X_train_neg))
    covariance_inverse_neg = np.linalg.inv(covariance_matrix_neg)
    total_count = positive_count+negative_count

    Y_test = np.empty([0],dtype=float)
    for X in X_test:
      	X_mean = np.add(X,-1*mean_positive)
      	X_value = np.matmul(X_mean,covariance_inverse_pos)
      	X_value = np.matmul(X_value,np.transpose(X_mean))
      	pos_class_prob = np.power((1/(2*np.pi)),feature_dimension/2)*np.exp((X_value)*(-1/2))*np.power(np.linalg.det(covariance_matrix_pos),-(1/2))

      	X_mean = np.add(X,-1*mean_negative)
      	X_value = np.matmul(X_mean,covariance_inverse_neg)
      	X_value = np.matmul(X_value,np.transpose(X_mean))
      	neg_class_prob = np.power((1/(2*np.pi)),feature_dimension/2)*np.exp((X_value)*(-1/2))*np.power(np.linalg.det(covariance_matrix_neg),-(1/2))

      	pos_posterior = (positive_count/total_count)*pos_class_prob/((positive_count/total_count*pos_class_prob)+(negative_count/total_count*neg_class_prob))
      	neg_posterior = (negative_count/total_count)*neg_class_prob/((positive_count/total_count*pos_class_prob)+(negative_count/total_count*neg_class_prob))

      	if pos_posterior >= threshold :
        	Y_test = np.append(Y_test,1)
      	else:
        	Y_test = np.append(Y_test,-1)
         
    return [X_train_pos,X_train_neg,Y_test,mean_positive,mean_negative,positive_count/total_count,negative_count/total_count,covariance_matrix_pos,covariance_matrix_neg]

Y_test_A_1_params = function_for_A_params(file_1['arr_0.npy'],file_1['arr_1.npy'],file_1['arr_2.npy'],0.5)
Y_test_B_1_params = function_for_B_params(file_1['arr_0.npy'],file_1['arr_1.npy'],file_1['arr_2.npy'],0.5)
Y_test_C_1_params = function_for_C_params(file_1['arr_0.npy'],file_1['arr_1.npy'],file_1['arr_2.npy'],0.5)
Y_test_A_2_params = function_for_A_params(file_2['arr_0.npy'],file_2['arr_1.npy'],file_2['arr_2.npy'],0.5)
Y_test_B_2_params = function_for_B_params(file_2['arr_0.npy'],file_2['arr_1.npy'],file_2['arr_2.npy'],0.5)
Y_test_C_2_params = function_for_C_params(file_2['arr_0.npy'],file_2['arr_1.npy'],file_2['arr_2.npy'],0.5)
Y_test_A_3_params = function_for_A_params(file_3['arr_0.npy'],file_3['arr_1.npy'],file_3['arr_2.npy'],0.5)
Y_test_B_3_params = function_for_B_params(file_3['arr_0.npy'],file_3['arr_1.npy'],file_3['arr_2.npy'],0.5)
Y_test_C_3_params = function_for_C_params(file_3['arr_0.npy'],file_3['arr_1.npy'],file_3['arr_2.npy'],0.5)

min1, max1 = file_1['arr_0.npy'][:, 0].min()-1, file_1['arr_0.npy'][:, 0].max()+1
min2, max2 = file_1['arr_0.npy'][:, 1].min()-1, file_1['arr_0.npy'][:, 1].max()+1
x1grid = np.arange(min1, max1, 0.05)
x2grid = np.arange(min2, max2, 0.05)
xx, yy = np.meshgrid(x1grid, x2grid)
r1, r2 = xx.flatten(), yy.flatten()
r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))
grid = np.hstack((r1,r2))


Z_A_A = function_for_A(file_1['arr_0.npy'],file_1['arr_1.npy'],grid)
Z_A_A = np.reshape(Z_A_A,(((np.shape(x2grid))[0]),((np.shape(x1grid))[0])))
plt.figure(1)
plt.contourf(xx,yy,Z_A_A,levels=2,colors=['#FE886E','#AFFE6E'])
plt.scatter(Y_test_A_1_params[0][:,0],Y_test_A_1_params[0][:,1],color="green")
plt.scatter(Y_test_A_1_params[1][:,0],Y_test_A_1_params[1][:,1],color="red")
plt.title("Bayesian A for Dataset A")
plt.savefig("Boundary_A_A.jpg")
plt.show()

Z_B_A = function_for_B(file_1['arr_0.npy'],file_1['arr_1.npy'],grid)
Z_B_A = np.reshape(Z_B_A,(((np.shape(x2grid))[0]),((np.shape(x1grid))[0])))
plt.figure(2)
plt.contourf(xx,yy,Z_B_A,levels=2,colors=['#FE886E','#AFFE6E'])
plt.scatter(Y_test_B_1_params[0][:,0],Y_test_B_1_params[0][:,1],color="green")
plt.scatter(Y_test_B_1_params[1][:,0],Y_test_B_1_params[1][:,1],color="red")
plt.title("Bayesian B for Dataset A")
plt.savefig("Boundary_B_A.jpg")
plt.show()

Z_C_A = function_for_C(file_1['arr_0.npy'],file_1['arr_1.npy'],grid)
Z_C_A = np.reshape(Z_C_A,(((np.shape(x2grid))[0]),((np.shape(x1grid))[0])))
plt.figure(3)
plt.contourf(xx,yy,Z_C_A,levels=2,colors=['#FE886E','#AFFE6E'])
plt.scatter(Y_test_C_1_params[0][:,0],Y_test_C_1_params[0][:,1],color="green")
plt.scatter(Y_test_C_1_params[1][:,0],Y_test_C_1_params[1][:,1],color="red")
plt.title("Bayesian C for Dataset A")
plt.savefig("Boundary_C_A.jpg")
plt.show()


min1, max1 = file_2['arr_0.npy'][:, 0].min()-1, file_2['arr_0.npy'][:, 0].max()+1
min2, max2 = file_2['arr_0.npy'][:, 1].min()-1, file_2['arr_0.npy'][:, 1].max()+1
x1grid = np.arange(min1, max1, 0.05)
x2grid = np.arange(min2, max2, 0.05)
xx, yy = np.meshgrid(x1grid, x2grid)
r1, r2 = xx.flatten(), yy.flatten()
r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))
grid = np.hstack((r1,r2))

Z_A_B = function_for_A(file_2['arr_0.npy'],file_2['arr_1.npy'],grid)
Z_A_B = np.reshape(Z_A_B,(((np.shape(x2grid))[0]),((np.shape(x1grid))[0])))
plt.figure(4)
plt.contourf(xx,yy,Z_A_B,levels=2,colors=['#FE886E','#AFFE6E'])
plt.scatter(Y_test_A_2_params[0][:,0],Y_test_A_2_params[0][:,1],color="green")
plt.scatter(Y_test_A_2_params[1][:,0],Y_test_A_2_params[1][:,1],color="red")
plt.title("Bayesian A for Dataset B")
plt.savefig("Boundary_A_B.jpg")
plt.show()

Z_B_B = function_for_B(file_2['arr_0.npy'],file_2['arr_1.npy'],grid)
Z_B_B = np.reshape(Z_B_B,(((np.shape(x2grid))[0]),((np.shape(x1grid))[0])))
plt.figure(5)
plt.contourf(xx,yy,Z_B_B,levels=2,colors=['#FE886E','#AFFE6E'])
plt.scatter(Y_test_B_2_params[0][:,0],Y_test_B_2_params[0][:,1],color="green")
plt.scatter(Y_test_B_2_params[1][:,0],Y_test_B_2_params[1][:,1],color="red")
plt.title("Bayesian B for Dataset B")
plt.savefig("Boundary_B_B.jpg")
plt.show()

Z_C_B = function_for_C(file_2['arr_0.npy'],file_2['arr_1.npy'],grid)
Z_C_B = np.reshape(Z_C_B,(((np.shape(x2grid))[0]),((np.shape(x1grid))[0])))
plt.figure(6)
plt.contourf(xx,yy,Z_C_B,levels=2,colors=['#FE886E','#AFFE6E'])
plt.scatter(Y_test_C_2_params[0][:,0],Y_test_C_2_params[0][:,1],color="green")
plt.scatter(Y_test_C_2_params[1][:,0],Y_test_C_2_params[1][:,1],color="red")
plt.title("Bayesian C for Dataset B")
plt.savefig("Boundary_C_B.jpg")
plt.show()


min1, max1 = file_3['arr_0.npy'][:, 0].min()-1, file_3['arr_0.npy'][:, 0].max()+1
min2, max2 = file_3['arr_0.npy'][:, 1].min()-1, file_3['arr_0.npy'][:, 1].max()+1
x1grid = np.arange(min1, max1, 0.05)
x2grid = np.arange(min2, max2, 0.05)
xx, yy = np.meshgrid(x1grid, x2grid)
r1, r2 = xx.flatten(), yy.flatten()
r1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))
grid = np.hstack((r1,r2))

Z_A_C = function_for_A(file_3['arr_0.npy'],file_3['arr_1.npy'],grid)
Z_A_C = np.reshape(Z_A_C,(((np.shape(x2grid))[0]),((np.shape(x1grid))[0])))
plt.figure(7)
plt.contourf(xx,yy,Z_A_C,levels=2,colors=['#FE886E','#AFFE6E'])
plt.scatter(Y_test_A_3_params[0][:,0],Y_test_A_3_params[0][:,1],color="green")
plt.scatter(Y_test_A_3_params[1][:,0],Y_test_A_3_params[1][:,1],color="red")
plt.title("Bayesian A for Dataset C")
plt.savefig("Boundary_A_C.jpg")
plt.show()

Z_B_C = function_for_B(file_3['arr_0.npy'],file_3['arr_1.npy'],grid)
Z_B_C = np.reshape(Z_B_C,(((np.shape(x2grid))[0]),((np.shape(x1grid))[0])))
plt.figure(8)
plt.contourf(xx,yy,Z_B_C,levels=2,colors=['#FE886E','#AFFE6E'])
plt.scatter(Y_test_B_3_params[0][:,0],Y_test_B_3_params[0][:,1],color="green")
plt.scatter(Y_test_B_3_params[1][:,0],Y_test_B_3_params[1][:,1],color="red")
plt.title("Bayesian B for Dataset C")
plt.savefig("Boundary_B_C.jpg")
plt.show()

Z_C_C = function_for_C(file_3['arr_0.npy'],file_3['arr_1.npy'],grid)
Z_C_C = np.reshape(Z_C_C,(((np.shape(x2grid))[0]),((np.shape(x1grid))[0])))
plt.figure(9)
plt.contourf(xx,yy,Z_C_C,levels=2,colors=['#FE886E','#AFFE6E'])
plt.scatter(Y_test_C_3_params[0][:,0],Y_test_C_3_params[0][:,1],color="green")
plt.scatter(Y_test_C_3_params[1][:,0],Y_test_C_3_params[1][:,1],color="red")
plt.title("Bayesian C for Dataset C")
plt.savefig("Boundary_C_C.jpg")
plt.show()

"""####6b) Give the ROC Curves for all the classifiers.


"""

# Cell type : CodeWrite
# write the code for loading the data, running the three algos, and plotting here. 
# (Use the functions written previously.)

threshold_vector = np.linspace(0,1,100)
FP=[]
TP=[]
for threshold in threshold_vector:
  FP_count,TP_count = 0,0
  FN_count,TN_count = 0,0
  Y_test_A_1_params = function_for_A_params(file_1['arr_0.npy'],file_1['arr_1.npy'],file_1['arr_2.npy'],threshold)
  for (y_predict,y_true) in zip(Y_test_A_1_params[2],file_1['arr_3.npy']):
    if y_predict == 1.0 and y_true == 1:
      TP_count += 1
    elif y_predict == 1.0 and y_true == -1:
      FP_count += 1
    elif y_predict == -1.0 and y_true == 1:
      FN_count += 1
    else:
      TN_count += 1
  try:
    FP.append(FP_count/(FP_count+TN_count))
    TP.append(TP_count/(TP_count+FN_count))
  except:
    pass
plt.figure(10)
plt.plot(FP,TP)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("Bayesian model A for dataset A")
plt.savefig("A-A.png")
plt.show()

FP=[]
TP=[]
for threshold in threshold_vector:
  FP_count,TP_count = 0,0
  FN_count,TN_count = 0,0
  Y_test_A_1_params = function_for_B_params(file_1['arr_0.npy'],file_1['arr_1.npy'],file_1['arr_2.npy'],threshold)
  for (y_predict,y_true) in zip(Y_test_A_1_params[2],file_1['arr_3.npy']):
    if y_predict == 1.0 and y_true == 1:
      TP_count += 1
    elif y_predict == 1.0 and y_true == -1:
      FP_count += 1
    elif y_predict == -1.0 and y_true == 1:
      FN_count += 1
    else:
      TN_count += 1
  try:
    FP.append(FP_count/(FP_count+TN_count))
    TP.append(TP_count/(TP_count+FN_count))
  except:
    pass
plt.figure(11)
plt.plot(FP,TP)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("Bayesian model B for dataset A")
plt.savefig("B-A.png")
plt.show()

FP=[]
TP=[]
for threshold in threshold_vector:
  FP_count,TP_count = 0,0
  FN_count,TN_count = 0,0
  Y_test_A_1_params = function_for_C_params(file_1['arr_0.npy'],file_1['arr_1.npy'],file_1['arr_2.npy'],threshold)
  for (y_predict,y_true) in zip(Y_test_A_1_params[2],file_1['arr_3.npy']):
    if y_predict == 1.0 and y_true == 1:
      TP_count += 1
    elif y_predict == 1.0 and y_true == -1:
      FP_count += 1
    elif y_predict == -1.0 and y_true == 1:
      FN_count += 1
    else:
      TN_count += 1
  try:
    FP.append(FP_count/(FP_count+TN_count))
    TP.append(TP_count/(TP_count+FN_count))
  except:
    pass
plt.figure(12)
plt.plot(FP,TP)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("Bayesian model C for dataset A")
plt.savefig("C-A.png")
plt.show()

FP=[]
TP=[]
for threshold in threshold_vector:
  FP_count,TP_count = 0,0
  FN_count,TN_count = 0,0
  Y_test_A_1_params = function_for_A_params(file_2['arr_0.npy'],file_2['arr_1.npy'],file_2['arr_2.npy'],threshold)
  for (y_predict,y_true) in zip(Y_test_A_1_params[2],file_2['arr_3.npy']):
    if y_predict == 1.0 and y_true == 1:
      TP_count += 1
    elif y_predict == 1.0 and y_true == -1:
      FP_count += 1
    elif y_predict == -1.0 and y_true == 1:
      FN_count += 1
    else:
      TN_count += 1
  try:
    FP.append(FP_count/(FP_count+TN_count))
    TP.append(TP_count/(TP_count+FN_count))
  except:
    pass
plt.figure(13)
plt.plot(FP,TP)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("Bayesian model A for dataset B")
plt.savefig("A-B.png")
plt.show()

FP=[]
TP=[]
for threshold in threshold_vector:
  FP_count,TP_count = 0,0
  FN_count,TN_count = 0,0
  Y_test_A_1_params = function_for_B_params(file_2['arr_0.npy'],file_2['arr_1.npy'],file_2['arr_2.npy'],threshold)
  for (y_predict,y_true) in zip(Y_test_A_1_params[2],file_2['arr_3.npy']):
    if y_predict == 1.0 and y_true == 1:
      TP_count += 1
    elif y_predict == 1.0 and y_true == -1:
      FP_count += 1
    elif y_predict == -1.0 and y_true == 1:
      FN_count += 1
    else:
      TN_count += 1
  try:
    FP.append(FP_count/(FP_count+TN_count))
    TP.append(TP_count/(TP_count+FN_count))
  except:
    pass
plt.figure(14)
plt.plot(FP,TP)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("Bayesian model B for dataset B")
plt.savefig("B-B.png")
plt.show()

FP=[]
TP=[]
for threshold in threshold_vector:
  FP_count,TP_count = 0,0
  FN_count,TN_count = 0,0
  Y_test_A_1_params = function_for_C_params(file_2['arr_0.npy'],file_2['arr_1.npy'],file_2['arr_2.npy'],threshold)
  for (y_predict,y_true) in zip(Y_test_A_1_params[2],file_2['arr_3.npy']):
    if y_predict == 1.0 and y_true == 1:
      TP_count += 1
    elif y_predict == 1.0 and y_true == -1:
      FP_count += 1
    elif y_predict == -1.0 and y_true == 1:
      FN_count += 1
    else:
      TN_count += 1
  try:
    FP.append(FP_count/(FP_count+TN_count))
    TP.append(TP_count/(TP_count+FN_count))
  except:
    pass
plt.figure(15)
plt.plot(FP,TP)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("Bayesian model C for dataset B")
plt.savefig("C-B.png")
plt.show()

FP=[]
TP=[]
for threshold in threshold_vector:
  FP_count,TP_count = 0,0
  FN_count,TN_count = 0,0
  Y_test_A_1_params = function_for_A_params(file_3['arr_0.npy'],file_3['arr_1.npy'],file_3['arr_2.npy'],threshold)
  for (y_predict,y_true) in zip(Y_test_A_1_params[2],file_3['arr_3.npy']):
    if y_predict == 1.0 and y_true == 1:
      TP_count += 1
    elif y_predict == 1.0 and y_true == -1:
      FP_count += 1
    elif y_predict == -1.0 and y_true == 1:
      FN_count += 1
    else:
      TN_count += 1
  try:
    FP.append(FP_count/(FP_count+TN_count))
    TP.append(TP_count/(TP_count+FN_count))
  except:
    pass
plt.figure(16)
plt.plot(FP,TP)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("Bayesian model A for dataset C")
plt.savefig("A-C.png")
plt.show()

FP=[]
TP=[]
for threshold in threshold_vector:
  FP_count,TP_count = 0,0
  FN_count,TN_count = 0,0
  Y_test_A_1_params = function_for_B_params(file_3['arr_0.npy'],file_3['arr_1.npy'],file_3['arr_2.npy'],threshold)
  for (y_predict,y_true) in zip(Y_test_A_1_params[2],file_3['arr_3.npy']):
    if y_predict == 1.0 and y_true == 1:
      TP_count += 1
    elif y_predict == 1.0 and y_true == -1:
      FP_count += 1
    elif y_predict == -1.0 and y_true == 1:
      FN_count += 1
    else:
      TN_count += 1
  try:
    FP.append(FP_count/(FP_count+TN_count))
    TP.append(TP_count/(TP_count+FN_count))
  except:
    pass
plt.figure(17)
plt.plot(FP,TP)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("Bayesian model B for dataset C")
plt.savefig("B-C.png")
plt.show()

FP=[]
TP=[]
for threshold in threshold_vector:
  FP_count,TP_count = 0,0
  FN_count,TN_count = 0,0
  Y_test_A_1_params = function_for_C_params(file_3['arr_0.npy'],file_3['arr_1.npy'],file_3['arr_2.npy'],threshold)
  for (y_predict,y_true) in zip(Y_test_A_1_params[2],file_3['arr_3.npy']):
    if y_predict == 1.0 and y_true == 1:
      TP_count += 1
    elif y_predict == 1.0 and y_true == -1:
      FP_count += 1
    elif y_predict == -1.0 and y_true == 1:
      FN_count += 1
    else:
      TN_count += 1
  try:
    FP.append(FP_count/(FP_count+TN_count))
    TP.append(TP_count/(TP_count+FN_count))
  except:
    pass
plt.figure(18)
plt.plot(FP,TP)
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.title("Bayesian model C for dataset C")
plt.savefig("C-C.png")
plt.show()

"""####6c) In the next Textwrite cell, give the error rate of the three classifiers on the three datasets as 3x3 table, with appropriately named rows and columns.

**Cell type : TextWrite**
(Write your observations and table of errors here)<br>
<pre>                        ERROR RATE TABLE
                            Dataset
              |   | A      | B      | C      |
              |---|--------|--------|--------|
              | A | 9.8%   | 50.84% | 11.75% |
Classifier    | B | 22.85% | 50.00% | 11.65% |
              | C | 22.55% | 7.45%  | 11.80% |
</pre>

####6d) In the next Textwrite cell, summarise your observations regarding the nine learnt classifiers.

**Cell type : TextWrite**
(Write your observations and table of errors here)
We see that as model C is more general in the sense that it has more parameters to optimize, it is consistently pretty accurate than model A and model B. This can be particularly seen in the data-set B in which as the variances vary greatly between the two types of data-points, the assumption of the variances being equal in A and B lead to big error rates.
"""